{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T15:10:42.474391Z",
     "start_time": "2018-10-19T15:10:41.771553Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "FILE_CATEGORY_TREE = '../data/category_tree.csv'\n",
    "FILE_EVENTS = '../data/events.csv'\n",
    "FILE_ITEM_PROPERTIES_1 = '../data/item_properties_part1.csv'\n",
    "FILE_ITEM_PROPERTIES_2 = '../data/item_properties_part2.csv'\n",
    "FILE_ITEM_PROPERTIES_ALL = '../data/item_properties_all.csv'\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original EDA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T22:07:47.729245Z",
     "start_time": "2018-10-17T22:07:47.717159Z"
    }
   },
   "source": [
    "``` python\n",
    "category_tree = pd.read_csv(FILE_CATEGORY_TREE)\n",
    "\n",
    "category_tree.head()\n",
    "\n",
    "category_tree.shape\n",
    "\n",
    "category_tree[category_tree.parentid.isnull()].sort_values('categoryid')\n",
    "\n",
    "def find_root(x, df):\n",
    "    while True: \n",
    "        if np.isnan(df.loc[df.categoryid == x, 'parentid'].unique()[0]):\n",
    "            return x\n",
    "        else:\n",
    "            x = df.loc[df.categoryid == x, 'parentid'].unique()[0]\n",
    "\n",
    "l = []\n",
    "for c in category_tree.categoryid:\n",
    "    l.append(find_root(c,category_tree))\n",
    "\n",
    "category_tree = pd.concat([category_tree,pd.Series(l,name='top_parent')],axis=1)\n",
    "\n",
    "top_parent_summary = category_tree.groupby('top_parent')['categoryid'].count().reset_index()\n",
    "top_parent_summary.rename(columns = {'categoryid':'num_categories'}, inplace=True)\n",
    "top_parent_summary.sort_values('num_categories',ascending=False)\n",
    "\n",
    "top_parent_summary.shape\n",
    "\n",
    "25 top-level categories \n",
    "\n",
    "events = pd.read_csv(FILE_EVENTS)\n",
    "\n",
    "events.head()\n",
    "\n",
    "events.describe(include='all')\n",
    "\n",
    "events.info()\n",
    "\n",
    "events.event.value_counts()\n",
    "\n",
    "events.itemid.unique().shape\n",
    "\n",
    "235,061 products view/addtocart/transcation\n",
    "\n",
    "def convert_to_local(x):\n",
    "    return datetime.fromtimestamp(x/1000)\n",
    "\n",
    "events['local_date_time'] = events.timestamp.apply(convert_to_local)\n",
    "\n",
    "events[events.visitorid == 1150086].sort_values('local_date_time').head(10)\n",
    "\n",
    "events.sort_values(['visitorid','local_date_time'], inplace=True)\n",
    "events['time_diff'] = events.groupby('visitorid')['timestamp'].diff(periods=-1) *-1\n",
    "\n",
    "events.time_diff = events.time_diff / 1000 # convert from milliseconds to seconds\n",
    "\n",
    "events.groupby('visitorid')['time_diff'].agg(['mean','count']).reset_index().sort_values('count', ascending=False).head()\n",
    "\n",
    "events[events.visitorid == 280150].sort_values('local_date_time').head(10)\n",
    "\n",
    "events.agg({'local_date_time':['min','max']})\n",
    "\n",
    "item_properties = pd.read_csv(FILE_ITEM_PROPERTIES_1)\n",
    "\n",
    "item_properties['local_date_time'] = item_properties.timestamp.apply(convert_to_local)\n",
    "\n",
    "item_properties[item_properties.itemid.isin([133542])].sort_values('timestamp')\n",
    "\n",
    "item_properties_2 = pd.read_csv(FILE_ITEM_PROPERTIES_2)\n",
    "\n",
    "item_properties_2['local_date_time'] = item_properties_2.timestamp.apply(convert_to_local)\n",
    "\n",
    "item_properties_2[item_properties_2.itemid.isin([133542])].sort_values('timestamp')\n",
    "\n",
    "item_properties_master = item_properties.append(item_properties_2)\n",
    "\n",
    "item_properties_master[item_properties_master.itemid.isin([133542])].sort_values(['property','local_date_time'])\n",
    "\n",
    "item_properties_master[item_properties_master.itemid.isin([167873])].sort_values(['property','local_date_time'])\n",
    "\n",
    "item_property_unique = item_properties_master.loc[:,['itemid','property']].drop_duplicates()\n",
    "\n",
    "property_count = item_property_unique.groupby('property')['itemid'].count().sort_values(ascending=False).reset_index()\n",
    "\n",
    "property_count[property_count.itemid == 417053]\n",
    "\n",
    "Above properties are represented of all items. Let's see what the unique values are for these properties.\n",
    "\n",
    "item_properties_master.loc[item_properties_master.property == '364','value'].value_counts().sort_values(ascending=False).reset_index().head(10)\n",
    "\n",
    "item_properties_master[item_properties_master.value.str.contains(r'[^\\s]')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New EDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T15:10:55.535278Z",
     "start_time": "2018-10-19T15:10:48.592820Z"
    }
   },
   "outputs": [],
   "source": [
    "events = pd.read_csv('../data/events.csv')\n",
    "events.local_date_time = pd.to_datetime(events.local_date_time)\n",
    "\n",
    "# set a row number primary key\n",
    "# events['pk'] = events.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T01:55:32.421638Z",
     "start_time": "2018-10-19T01:55:32.391198Z"
    }
   },
   "outputs": [],
   "source": [
    "category_tree = pd.read_csv('../data/category_tree_parent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T01:56:11.780722Z",
     "start_time": "2018-10-19T01:55:32.427651Z"
    }
   },
   "outputs": [],
   "source": [
    "item_properties_master = pd.read_csv('../data/item_properties_master.csv')\n",
    "item_properties_master.local_date_time = pd.to_datetime(item_properties_master.local_date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce data set size and begin to build MVP feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|visitorid|counts|\n",
    "---|---|\n",
    "|152963 |    2054\n",
    "|994820  |   1661\n",
    "|1150086  |  1524\n",
    "|247235    | 1425\n",
    "|645525     |1411\n",
    "|79627      |1180\n",
    "|530559     |1091\n",
    "|737053     |1055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature building\n",
    "\n",
    "1. Calcualte session id for the events\n",
    "2. Calcualte number of views in each session\n",
    "3. Calcualte total session length\n",
    "4. Build design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T19:46:59.313006Z",
     "start_time": "2018-10-19T19:44:15.760875Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate Session ID\n",
    "\n",
    "def calc_session_id(df, mask):\n",
    "    df['session_id'] = np.nan\n",
    "    ind = df.groupby('visitorid').head(1).index\n",
    "    df.loc[ind, 'session_id'] = 1\n",
    "\n",
    "    count_session = df[mask].shape[0] + 1\n",
    "    df.loc[mask, 'session_id'] = np.arange(2,count_session+1)\n",
    "\n",
    "    # fill in all of the gaps\n",
    "    df.session_id.fillna(method = 'ffill', inplace=True)\n",
    "\n",
    "    # make the session id unique\n",
    "    df.session_id = df.visitorid.astype(str) + '_' + df.session_id.astype(int).astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_to_local(x):\n",
    "    return datetime.fromtimestamp(x/1000)\n",
    "\n",
    "# get data\n",
    "events = pd.read_csv('../data/events.csv')\n",
    "events['local_date_time'] = events.timestamp.apply(convert_to_local)\n",
    "\n",
    "# reduce data set size for MVP\n",
    "events_trimmed = events[events.local_date_time >= datetime(2015, 8, 15)]\n",
    "\n",
    "print(f'Total number of events: {events_trimmed.shape[0]:,}')\n",
    "print()\n",
    "\n",
    "# first calcualte sessions for each buy transaction\n",
    "events_trimmed.sort_values(['visitorid','local_date_time'], inplace=True)\n",
    "events_trimmed['prev_event'] = events_trimmed.groupby('visitorid').event.shift(1)\n",
    "sub = (events_trimmed.event == 'view') & (events_trimmed.prev_event == 'transaction')\n",
    "events_trimmed = calc_session_id(events_trimmed, sub)\n",
    "\n",
    "print(f'Total number of sessions dividing on transaction: {len(events_trimmed.session_id.unique()):,}')\n",
    "print()\n",
    "\n",
    "# calcualte the time diff within each session\n",
    "events_trimmed.sort_values(['session_id', 'local_date_time'], inplace = True)\n",
    "\n",
    "events_trimmed['time_diff'] = (events_trimmed\n",
    "                               .groupby('session_id')['timestamp']\n",
    "                               .diff(1)\n",
    "                               .fillna(0) \n",
    "                               / 1000)\n",
    "\n",
    "# events_trimmed.['time_diff'] = events_trimmed.time_diff / 1000\n",
    "events_trimmed['page_length'] = events_trimmed.groupby('visitorid').time_diff.shift(-1)\n",
    "\n",
    "# re-calaculate sessions with a new session starting whenever a buy occurs or if a view lasts longer than 3.5 minutes\n",
    "session_time_limit = 600 ## Should consider building a time limit per user, instead of global\n",
    "\n",
    "sub = (((events_trimmed.event == 'view') \n",
    "       & (events_trimmed.prev_event == 'transaction'))\n",
    "      | ((events_trimmed.event == 'view')\n",
    "        & (events_trimmed.time_diff > session_time_limit)))\n",
    "\n",
    "events_trimmed = calc_session_id(events_trimmed, sub)\n",
    "\n",
    "print(f'Total number of sessions adding a time limit: {len(events_trimmed.session_id.unique()):,}')\n",
    "print()\n",
    "\n",
    "## Calcualte total views per session\n",
    "\n",
    "view_counts_df = (events_trimmed[events_trimmed.event == 'view']\n",
    "                     .groupby('session_id')['event']\n",
    "                     .count())\n",
    "view_counts_df.name = 'view_count'\n",
    "\n",
    "## Calcualte total session length per session\n",
    "\n",
    "session_length_df = (events_trimmed\n",
    "                        .groupby('session_id')['page_length']\n",
    "                        .agg(['sum','mean']))\n",
    "session_length_df.rename(columns = {'sum':'session_length','mean':'avg_len_per_pg'}, inplace=True)\n",
    "\n",
    "## Build Design Matrix\n",
    "\n",
    "events_trimmed['buy_event'] = 0\n",
    "events_trimmed.loc[events_trimmed.event == 'transaction', 'buy_event'] = 1\n",
    "\n",
    "buy_event = events_trimmed.groupby('session_id')['buy_event'].max()\n",
    "\n",
    "print(f'Buy event observations: {len(buy_event):,}')\n",
    "print()\n",
    "\n",
    "design_df = pd.concat([buy_event, view_counts_df, session_length_df],axis=1, sort=True)\n",
    "design_df = design_df.fillna(0)\n",
    "\n",
    "print(f'Design observations: {design_df.shape[0]:,}')\n",
    "print()\n",
    "\n",
    "print(f'Precentage of buys: {len(design_df[design_df.buy_event >= 1].buy_event) / len(design_df.buy_event):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T19:42:37.967395Z",
     "start_time": "2018-10-19T19:42:37.780450Z"
    }
   },
   "outputs": [],
   "source": [
    "events_trimmed[events_trimmed.visitorid == 152963]\n",
    "# .session_id.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 mins per page - _Using this setting_\n",
    "Total number of events: 599,871\n",
    "\n",
    "Total number of sessions dividing on transaction: 337,455\n",
    "\n",
    "Total number of sessions adding a time limit: 415,948\n",
    "\n",
    "Buy event observations: 415,948\n",
    "\n",
    "Design observations: 415,948\n",
    "\n",
    "Precentage of buys: 0.93%\n",
    "\n",
    "## 3.5 mins per page\n",
    "Total number of events: 599,871\n",
    "\n",
    "Total number of sessions dividing on transaction: 337,455\n",
    "\n",
    "Total number of sessions adding a time limit: 444,864\n",
    "\n",
    "Buy event observations: 444,864\n",
    "\n",
    "Design observations: 444,864\n",
    "\n",
    "Precentage of buys: 0.87%\n",
    "\n",
    "\n",
    "## 10 mins per page\n",
    "Total number of events: 599,871\n",
    "\n",
    "Total number of sessions dividing on transaction: 337,455\n",
    "\n",
    "Total number of sessions adding a time limit: 404,052\n",
    "\n",
    "Buy event observations: 404,052\n",
    "\n",
    "Design observations: 404,052\n",
    "\n",
    "Precentage of buys: 0.96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:58:47.874403Z",
     "start_time": "2018-10-19T16:58:47.868042Z"
    }
   },
   "outputs": [],
   "source": [
    "print(design_df.shape)\n",
    "print(len(buy_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:04:27.223881Z",
     "start_time": "2018-10-19T17:04:27.201278Z"
    }
   },
   "outputs": [],
   "source": [
    "events_trimmed[events_trimmed.visitorid == 1000675]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:14:14.477420Z",
     "start_time": "2018-10-19T17:14:14.464570Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:14:15.712288Z",
     "start_time": "2018-10-19T17:14:15.344615Z"
    }
   },
   "outputs": [],
   "source": [
    "y = design_df.buy_event\n",
    "X = design_df.drop(columns='buy_event')\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:14:19.197461Z",
     "start_time": "2018-10-19T17:14:15.985094Z"
    }
   },
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:14:26.969773Z",
     "start_time": "2018-10-19T17:14:26.813430Z"
    }
   },
   "outputs": [],
   "source": [
    "log_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:14:28.307653Z",
     "start_time": "2018-10-19T17:14:28.298228Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred = log_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:14:28.863599Z",
     "start_time": "2018-10-19T17:14:28.860223Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T17:14:30.850006Z",
     "start_time": "2018-10-19T17:14:30.756728Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:21:34.112233Z",
     "start_time": "2018-10-18T22:21:32.561394Z"
    }
   },
   "outputs": [],
   "source": [
    "property_df = item_properties_master[item_properties_master.property == 'categoryid']\n",
    "property_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:25:56.352536Z",
     "start_time": "2018-10-18T22:25:55.645854Z"
    }
   },
   "outputs": [],
   "source": [
    "events_trimmed.sort_values('local_date_time', inplace=True)\n",
    "property_df.sort_values('local_date_time', inplace=True)\n",
    "\n",
    "print(f'events_trimmed rows: {events_trimmed.shape[0]:,}')\n",
    "events_trimmed_property = pd.merge_asof(events_trimmed, property_df, on=\"local_date_time\", by=\"itemid\") \n",
    "print(f'merged rows: {events_trimmed_property.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T22:30:36.237851Z",
     "start_time": "2018-10-18T22:30:36.006676Z"
    }
   },
   "outputs": [],
   "source": [
    "events_trimmed_property.dropna(subset=['value']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:41:35.930195Z",
     "start_time": "2018-10-18T15:41:35.922302Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make this into a function so I can get any property\n",
    "def set_item_property(property_master_df, feature_df, prop, property_name ):\n",
    "    # filter down to the property\n",
    "    property_df = property_master_df[property_master_df.property == prop]\n",
    "\n",
    "    # get all property rows for each item in the feature data set\n",
    "    merge_event_property_df = feature_df.merge(property_df, how='left',on='itemid')\n",
    "    print(f'Rows when left join {merge_event_property_df.shape[0]:,}')\n",
    "\n",
    "    merge_event_property_df = feature_df.merge(property_df, how='inner',on='itemid')\n",
    "    print(f'Rows when inner join {merge_event_property_df.shape[0]:,}')\n",
    "\n",
    "    print('Using inner join for now, and will come back later.')\n",
    "\n",
    "    # remove all rows where the property was updated after the event timestamp\n",
    "    print(f'Nuber of items: {len(merge_event_property_df.itemid.)}')\n",
    "    merge_event_property_df = (merge_event_property_df[merge_event_property_df.local_date_time_x \n",
    "                                                       > merge_event_property_df.local_date_time_y])\n",
    "\n",
    "    property_max_date_time = (merge_event_property_df\n",
    "                              .groupby(['local_date_time_x','itemid','session_id'])['local_date_time_y']\n",
    "                              .max()\n",
    "                              .reset_index())\n",
    "    \n",
    "    merge_event_property_df = (merge_event_property_df\n",
    "                               .merge(property_max_date_time\n",
    "                                      , how='inner'\n",
    "                                      , on=['local_date_time_x','itemid','session_id','local_date_time_y']))\n",
    "    print(f'Rows when inner join {merge_event_property_df.shape[0]:,}')\n",
    "\n",
    "    # clean up the df\n",
    "    drop_c = ['timestamp_y', 'property', 'local_date_time_y']\n",
    "    merge_event_property_df.drop(columns=drop_c, inplace=True)\n",
    "\n",
    "    rename_c = {'timestamp_x':'timestamp', 'local_date_time_x':'local_date_time','value':property_name}\n",
    "    merge_event_property_df.rename(columns=rename_c, inplace=True)\n",
    "\n",
    "    return merge_event_property_df, property_max_date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:10:56.530271Z",
     "start_time": "2018-10-19T03:10:56.364126Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "events_trimmed[events_trimmed.visitorid == 152963]\n",
    "# print(events_trimmed[(events_trimmed.visitorid == 152963) & (events_trimmed.time_diff > 210)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:07:16.900478Z",
     "start_time": "2018-10-19T02:07:16.894813Z"
    }
   },
   "outputs": [],
   "source": [
    "210 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:06:11.104361Z",
     "start_time": "2018-10-19T02:06:11.039499Z"
    }
   },
   "outputs": [],
   "source": [
    "events_trimmed.time_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-18T19:41:29.288Z"
    }
   },
   "outputs": [],
   "source": [
    "events_trimmed = events[events.local_date_time >= datetime(2015, 8, 15)]\n",
    "\n",
    "print(f'Trimmed events {events_trimmed.shape[0]:,}')\n",
    "print()\n",
    "\n",
    "print(f'Count of events: \\n{events_trimmed.event.value_counts()}')\n",
    "print()\n",
    "# unique visitors\n",
    "print(f'Visitors that bought something {events_trimmed[events_trimmed.event == \"transaction\"].visitorid.unique().shape[0]:,}')\n",
    "print()\n",
    "\n",
    "###### Probably remove this as we want to look at buy vs not buy\n",
    "\n",
    "# # all visitors where at least one session ended in a transaction\n",
    "# visitors = events_trimmed[events_trimmed.event == 'transaction'].visitorid.unique()\n",
    "# print(f'Events for visitors who bought something {events_trimmed[events_trimmed.visitorid.isin(visitors)].shape[0]:,}')\n",
    "# buy_visitors = events_trimmed[events_trimmed.visitorid.isin(visitors)]\n",
    "\n",
    "######\n",
    "\n",
    "# calculate the session_id\n",
    "# session_id identifes each pattern of view...transaction for each visitor as unique\n",
    "l = []\n",
    "for v in events_trimmed.visitorid.unique():\n",
    "    v_df = events_trimmed[events_trimmed.visitorid == v].sort_values('local_date_time')\n",
    "    prev_event = 'view'\n",
    "    session_id = 1\n",
    "    for i in v_df.index:\n",
    "        if prev_event == 'transaction' and v_df.loc[i,'event'] != 'transaction':\n",
    "            session_id += 1\n",
    "\n",
    "        prev_event = v_df.loc[i,'event']\n",
    "        l.append(session_id)\n",
    "\n",
    "# assign each session_id and make it unique\n",
    "events_trimmed['session_id'] = l\n",
    "events_trimmed.session_id = events_trimmed.visitorid.astype('str') + '_' + events_trimmed.session_id.astype('str')\n",
    "\n",
    "###### Probably remove this as we want to look at buy vs not buy\n",
    "\n",
    "# # group by session_id and remove those sessions without a transaction\n",
    "# grouped_events = buy_visitors.groupby(['session_id','event'])['visitorid'].count().reset_index()\n",
    "# valid_sessions = grouped_events[grouped_events.event == 'transaction'].session_id\n",
    "# buy_visitors = buy_visitors[buy_visitors.session_id.isin(valid_sessions)]\n",
    "\n",
    "# print()\n",
    "# print(f'Sessions that have at least one transaction {buy_visitors.shape[0]:,}')\n",
    "\n",
    "######\n",
    "\n",
    "# calcaulte session length feature\n",
    "events_trimmed.time_diff = events_trimmed.time_diff.shift(1)\n",
    "events_trimmed.rename(columns={'time_diff':'session_length'}, inplace=True)\n",
    "\n",
    "# calculate hour of day and day of week\n",
    "events_trimmed['session_hour'] = events_trimmed.local_date_time.dt.hour\n",
    "events_trimmed['session_dow'] = events_trimmed.local_date_time.dt.dayofweek\n",
    "\n",
    "# set the category id property\n",
    "# print()\n",
    "# print('Adding category')\n",
    "# buy_visitors, _ = set_item_property(item_properties_master, buy_visitors, 'categoryid', 'category_id' )\n",
    "# print(f'Updated shape of the feature DF {buy_visitors.shape[0]:,}')\n",
    "\n",
    "# set the available property\n",
    "# print()\n",
    "# print('Adding available')\n",
    "# buy_visitors, property_group_df = set_item_property(item_properties_master, buy_visitors, 'available', 'available' )\n",
    "# print(f'Updated shape of the feature DF {buy_visitors.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:47:31.474518Z",
     "start_time": "2018-10-18T15:47:31.457545Z"
    }
   },
   "outputs": [],
   "source": [
    "buy_visitors_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:58:00.537191Z",
     "start_time": "2018-10-18T15:58:00.515047Z"
    }
   },
   "outputs": [],
   "source": [
    "# buy_visitors.timestamp.value_counts().head()\n",
    "buy_visitors[buy_visitors.timestamp == '143991494102']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:56:07.003219Z",
     "start_time": "2018-10-18T15:55:52.240627Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def key_options(items):\n",
    "    return chain.from_iterable(combinations(items, r) for r in range(1, len(items)+1) )\n",
    "\n",
    "df = buy_visitors\n",
    "\n",
    "# iterate over all combos of headings, excluding ID for brevity\n",
    "for candidate in key_options(list(df)):\n",
    "    deduped = df.drop_duplicates(candidate)\n",
    "\n",
    "    if len(deduped.index) == len(df.index): #and len(deduped.index) <= 4:\n",
    "        print(','.join(candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:47:12.277417Z",
     "start_time": "2018-10-18T15:47:12.186619Z"
    }
   },
   "outputs": [],
   "source": [
    "buy_visitors_category.merge(buy_visitors_category_avail, how='left', on=['timestamp', 'visitorid','itemid','event']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:38:33.887874Z",
     "start_time": "2018-10-18T15:38:33.876990Z"
    }
   },
   "outputs": [],
   "source": [
    "property_group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:40:39.656712Z",
     "start_time": "2018-10-18T15:40:39.402568Z"
    }
   },
   "outputs": [],
   "source": [
    "(buy_visitors\n",
    "    .merge(property_group_df\n",
    "              , how='inner'\n",
    "              , on=['local_date_time_x','itemid','session_id','local_date_time_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T15:29:27.866691Z",
     "start_time": "2018-10-18T15:29:25.353152Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_properties_master.property.value_counts()\n",
    "# TODO Available and then figure out the t-1 thing.\n",
    "# Thinking of adding a feature to start with transaction and count backwards within each session ordered by time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
